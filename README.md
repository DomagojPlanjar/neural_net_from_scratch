# Neural Network from Scratch

## Overview

This project implements a neural network from scratch using NumPy, providing a hands-on understanding of core concepts in deep learning. The network includes various layers such as convolutional, fully connected, pooling, and activation functions. The project supports forward and backward passes, gradient checking, and includes L2 regularization.

## Features

- **Layers Implemented:**
  - Convolution
  - MaxPooling
  - ReLU
  - Fully Connected
  - Softmax Cross-Entropy with Logits
- **Regularization:**
  - L2 Regularization
- **Training and Evaluation:**
  - Functions for training, evaluating, and saving/loading models
- **Gradient Checking:**
  - Numerical gradient checking for verification

## Getting Started

### Prerequisites

- Python 3.x
- NumPy
- (Optional) PyTorch for comparison

### Installation

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/YourUsername/neural_net_from_scratch.git
   cd neural_net_from_scratch
